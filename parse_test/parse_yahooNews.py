from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import json
import time
import os
import datetime
import re

# ================= åƒæ•¸è¨­å®š =================
target_url = 'https://tw.news.yahoo.com/%E5%85%B1%E8%BB%8D%E6%BC%94%E7%BF%92%E4%BE%B5%E5%8F%B0%E7%81%A312%E6%B5%AC%E9%A0%98%E6%B5%B7-%E5%9C%8B%E9%98%B2%E9%83%A8%E6%8E%88%E6%AC%8A%E4%BD%9C%E6%88%B0%E5%96%AE%E4%BD%8D%E9%81%A9%E6%99%82%E6%87%89%E5%B0%8D-022759022.html'
output_folder = r"json_test/Yahoo/Article"
os.makedirs(output_folder, exist_ok=True)

def get_yahoo_content_v13(url):
    print(f"å•Ÿå‹•ç€è¦½å™¨ (V13 JS å¯„ç”Ÿç‰ˆ)...")
    
    chrome_options = Options()
    chrome_options.add_argument("--start-maximized")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled") 
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    
    final_data = {}

    try:
        driver.get(url)
        
        # === 1. ç¢ºä¿å…§æ–‡è¼‰å…¥ (ä¿®æ­£ 0 å­—å•é¡Œ) ===
        print("ç­‰å¾…é é¢è¼‰å…¥...")
        try:
            # ç­‰å¾…å…§æ–‡å€å¡Šå‡ºç¾
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, "caas-body")))
        except:
            print("âš ï¸ å…§æ–‡ç­‰å¾…é€¾æ™‚ï¼Œå¯èƒ½è¼‰å…¥è¼ƒæ…¢")

        time.sleep(2) # å¤šçµ¦ä¸€é»ç·©è¡æ™‚é–“è®“æ–‡å­—æ¸²æŸ“

        # è§£æå…§æ–‡
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        h1 = soup.find('h1')
        title = h1.get_text(strip=True) if h1 else "Unknown Title"
        content_div = soup.find('div', class_='caas-body')
        content_raw = "\n".join([p.get_text(strip=True) for p in content_div.find_all('p')]) if content_div else ""
        publish_time = soup.find('time')['datetime'] if soup.find('time') else ""
        
        print(f"âœ… å…§æ–‡æŠ“å–: {len(content_raw)} å­—")
        
        # === 2. è§¸ç™¼ç•™è¨€å€ (ç‚ºäº†æ‹¿åˆ° Crumb) ===
        print("æ»¾å‹•è‡³åº•éƒ¨è§¸ç™¼ç•™è¨€è¼‰å…¥...")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        
        # å®šä½ç•™è¨€ iframe
        iframe = WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, "//iframe[contains(@src, '/comments/')]"))
        )
        
        # å¾ iframe src è§£æ context ID
        src = iframe.get_attribute('src')
        match = re.search(r'context=([a-zA-Z0-9\-]+)', src)
        context_id = match.group(1) if match else None
        print(f"âœ… Context ID: {context_id}")

        # åˆ‡æ›é€² iframe æ‹¿ Crumb
        driver.switch_to.frame(iframe)
        time.sleep(1)
        
        iframe_html = driver.page_source
        crumb_match = re.search(r'"crumb"\s*:\s*"([^"]+)"', iframe_html)
        crumb = crumb_match.group(1) if crumb_match else None
        
        # æœ‰æ™‚å€™ crumb æœƒæœ‰è·³è„«å­—å…ƒï¼Œè™•ç†ä¸€ä¸‹
        if crumb: 
            crumb = crumb.encode().decode('unicode_escape')
            print(f"âœ… Crumb: {crumb}")
        
        # === 3. JS å¯„ç”Ÿæ”»æ“Š (æ ¸å¿ƒä¿®æ”¹) ===
        # æˆ‘å€‘ä¸åˆ‡å›ä¸»é é¢ï¼Œç›´æ¥åœ¨ iframe è£¡é¢åŸ·è¡Œ fetch
        # å› ç‚ºé€™å€‹ iframe çš„ç¶²åŸŸè·Ÿ API æ˜¯åŒæºçš„ (æˆ–æ˜¯å®ƒæ“æœ‰æ­£ç¢ºçš„ cookie æ¬Šé™)
        
        all_comments = []
        
        if context_id and crumb:
            print("ğŸš€ é–‹å§‹åŸ·è¡Œç€è¦½å™¨å…§å»º Fetch (JS)...")
            
            # å®šç¾©ä¸€å€‹ JS è…³æœ¬ï¼Œå®ƒæœƒè‡ªå‹•åˆ†é æŠ“å–æ‰€æœ‰ç•™è¨€
            # é€™æ®µ JS æœƒåœ¨ç€è¦½å™¨å…§éƒ¨åŸ·è¡Œ
            fetch_script = """
            var callback = arguments[arguments.length - 1];
            var contextId = arguments[0];
            var crumb = arguments[1];
            var allMessages = [];
            var offset = 0;
            var batchSize = 100;
            
            async function fetchAll() {
                while(true) {
                    // å»ºæ§‹ URL
                    var url = `https://tw.news.yahoo.com/_td-news/api/resource/canvass.getMessageListForContext_ns;context=${contextId};count=${batchSize};lang=zh-Hant-TW;sortBy=highestRated;index=v%3D1%3As%3DhighestRated%3Aoff%3D${offset}?crumb=${crumb}`;
                    
                    try {
                        var response = await fetch(url);
                        if (!response.ok) {
                            console.error("Fetch failed: " + response.status);
                            break;
                        }
                        var data = await response.json();
                        var messages = data.canvassMessages || [];
                        
                        if (messages.length === 0) break;
                        
                        // ç°¡åŒ–è³‡æ–™å›å‚³ï¼Œæ¸›å°‘å‚³è¼¸é‡
                        messages.forEach(msg => {
                            allMessages.push({
                                user: msg.details.userContext.nickname,
                                content: msg.details.userText,
                                time: msg.meta.createdAt,
                                likes: msg.reactionStats.count
                            });
                        });
                        
                        offset += messages.length;
                        // ç°¡å–®çš„é˜²å‘†ï¼Œé¿å…ç„¡é™è¿´åœˆï¼Œæœ€å¤šæŠ“ 1000 å‰‡
                        if (offset > 1000) break;
                        
                        // ç¨å¾®ä¼‘æ¯ä¸€ä¸‹é¿å…å¤ªå¿«
                        await new Promise(r => setTimeout(r, 500));
                        
                    } catch (e) {
                        console.error(e);
                        break;
                    }
                }
                callback(allMessages);
            }
            
            fetchAll();
            """
            
            # ä½¿ç”¨ execute_async_script åŸ·è¡Œä¸Šé¢çš„ JS
            # é€™æ˜¯ Selenium æœ€å¼·å¤§çš„åŠŸèƒ½ä¹‹ä¸€ï¼Œå¯ä»¥ç­‰å¾… JS è·‘å®Œæ‰å›å‚³ Python
            try:
                # è¨­å®šè…³æœ¬è¶…æ™‚æ™‚é–“ (å› ç‚ºè¦æŠ“å¾ˆå¤šé ï¼Œçµ¦å®ƒ 60 ç§’)
                driver.set_script_timeout(60)
                js_result = driver.execute_async_script(fetch_script, context_id, crumb)
                
                print(f"âœ… JS å›å‚³æˆåŠŸï¼å…±å–å¾— {len(js_result)} ç­†ç•™è¨€")
                
                # è½‰æ›è³‡æ–™æ ¼å¼
                for item in js_result:
                    all_comments.append({
                        "user_name": item['user'],
                        "content": item['content'],
                        "time": datetime.datetime.fromtimestamp(item['time']).strftime('%Y-%m-%d %H:%M:%S'),
                        "likes": item['likes'],
                        "reply_count": 0
                    })
                    
            except Exception as e:
                print(f"âŒ JS åŸ·è¡Œå¤±æ•—: {e}")

        # æ•´ç†æœ€çµ‚è³‡æ–™
        final_data = {
            "article_title": title,
            "article_url": url,
            "publish_time": publish_time,
            "content_raw": content_raw,
            "comment_count": len(all_comments),
            "comments": all_comments
        }

    except Exception as e:
        print(f"ä¸»è¦æµç¨‹éŒ¯èª¤: {e}")
    finally:
        driver.quit()
        
    return final_data

if __name__ == "__main__":
    data = get_yahoo_content_v13(target_url)
    
    if data:
        timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        filename = f"yahoo_news_v13_{timestamp}.json"
        filepath = os.path.join(output_folder, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            
        print(f"\næª”æ¡ˆå·²å„²å­˜: {filepath}")
        print(f"å…§æ–‡é•·åº¦: {len(data.get('content_raw', ''))}")
        print(f"ç•™è¨€æ•¸é‡: {data['comment_count']}")